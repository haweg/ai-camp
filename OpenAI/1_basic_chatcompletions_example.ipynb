{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e7451",
   "metadata": {},
   "source": [
    "# Creating basic chat completions\n",
    "\n",
    "Chat models are a class of GPT models that take a series of messages as input, and return a model-generated message as output. All newer GPT models released by OpenAI are chat models, including GPT-3.5-Turbo and GPT-4 (which are used in ChatGPT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914026b",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa2258",
   "metadata": {},
   "source": [
    "First we need to install the `openai` library in your environment. You can do this by running the `pip` command below. Typically, you would do this in a terminal or command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea128b",
   "metadata": {},
   "source": [
    "Next we need to import the `openai` library so we can use it in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbb9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947500e",
   "metadata": {},
   "source": [
    "Now we can use the `openai` library to configure the API endpoint and API key that we will use to access the Azure OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1a2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for using an Azure OpenAI endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = \"<your_api_key>\"                           # The API key for your Azure OpenAI resource\n",
    "openai.api_base = \"<your_base_url>\"                         # The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_version = \"2023-07-01-preview\"                   # The API version for your Azure OpenAI resource. e.g. \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33f92a",
   "metadata": {},
   "source": [
    "## 2. Using an Azure OpenAI resource to generate chat completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9685631",
   "metadata": {},
   "source": [
    "Now we are able to use the `openai` library to generate chat completions. First we have to configure the API endpoint and API key that we want to use. We will then use the `openai.ChatCompletion` class to generate chat completions.\n",
    "\n",
    "The main input is the `messages` parameter. Messages must be an array of message objects, where each object has a role (either `system`, `user`, or `assistant`) and content (the content of the message).\n",
    "- The first message is typically a `system` message, which contains global instructions and context for the model. You can use this to tell the model how to behave, how to interpret the user's messages or how to structure the response.\n",
    "- `user` messages typically contain user input or other information that the model should use to generate a response.\n",
    "- The models previous responses are stored as messages with the role `assistant`.\n",
    "\n",
    "You also have to specify the `deployment_id` parameter (or, when using the native OpenAI API, the `model` parameter). This is the ID of the model you want to use (e.g. `gpt-4`).\n",
    "\n",
    "Other than that, you can specify multiple optional parameters to control the behavior of the model. For example, you can specify the `max_tokens` parameter to control the length of the response, or the `temperature` parameter to control the creativity of the response. You can find a full list of parameters in the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions) (or in the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d67d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie, born Maria Sk≈Çodowska, was a Polish-born physicist and chemist who became one of the most famous scientists in history. She is best known for her pioneering research on radioactivity, which led to the discovery of two new elements, polonium and radium. Curie was the first woman to win a Nobel Prize and the only person to win Nobel Prizes in two different scientific fields (Physics and Chemistry). Her groundbreaking work laid the foundation for many advancements in the field of nuclear physics and medicine. Curie's contributions to science and her perseverance in the face of gender discrimination have made her an inspirational figure for generations.\n"
     ]
    }
   ],
   "source": [
    "# a sample API call for chat completions looks as follows:\n",
    "try:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id = \"gpt-35-turbo\", # The deployment ID for your Azure OpenAI resource\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who is Marie Curie?\"},\n",
    "        ],\n",
    "        temperature = 0.2,              # Number between 0.0 and 2.0. Higher values will make the output more random, while lower values will make it more focused and deterministic. It is recommended altering this or top_p but not both.\n",
    "        n = 1,                          # How many completion choices to generate on each call.\n",
    "        stop = None,                    # One or more sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "        max_tokens = 150,               # The maximum number of tokens to generate.\n",
    "        stream = False,                 # Whether to stream back partial progress. If set, tokens will be returned one-by-one as they are generated, using multiple API calls if necessary.\n",
    "        # top_p = 1,                    # Number between 0.0 and 1.0. Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. It is recommended altering this or temperature but not both.\n",
    "        # presence_penalty = 0.0,       # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "        # frequency_penalty = 0.0,      # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "    )\n",
    "    # print the response\n",
    "    for choice in response['choices']:\n",
    "        print(choice['message']['content'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
