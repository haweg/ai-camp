{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providing functions for the GPT model to use\n",
    "This sample notebook demonstrates how you can provide a GPT model with custom functions to use. This can be useful to enable a model to perform tasks that require interaction with external services, such as retrieving additional information from a database, performing more complex calculations or using any other tool that you want to integrate with the model. <br>\n",
    "This example builds on the previous notebook about managing GPT conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** This notebook requires a newer model version of GPT-3.5-Turbo or GPT-4, which is not yet available in the Azure OpenAI resource behind the proxy `apis.ai.cosmoconsult.com` from our previous examples.\n",
    "Instead, use the endpoint `cosmoai-hans.openai.azure.com` for this notebook. You can request an API key to this endpoint by sending an email to [Hans Wegener](mailto:hans.wegener@cosmoconsult.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade \"openai<1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the tiktoken Python library\n",
    "%pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI and tiktoken Python library\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for using an Azure OpenAI endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = \"<your_api_key>\"                           # The API key for your Azure OpenAI resource -> Get yours from https://ai.cosmoconsult.com/me\n",
    "openai.api_base = \"https://cosmoai-hans.openai.azure.com\"   # The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_version = \"2023-07-01-preview\"                   # The API version for your Azure OpenAI resource. e.g. \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are going to provide the model with a simple function that can retrieve the weather for a given location and date. To be able to do this, we first need define a helper function in Python that can retrieve the weather from a weather API. We will use the [Open-Meteo API](https://open-meteo.com/en/docs) to retrieve the weather data for a given latitude, longitude and date. The function will return the raw JSON response from the API ([example](https://api.open-meteo.com/v1/forecast?latitude=51.0509&longitude=13.7383&daily=weathercode,temperature_2m_max,temperature_2m_min,precipitation_sum,precipitation_hours,windspeed_10m_max,windgusts_10m_max&current_weather=true&timezone=GMT&forecast_days=1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude, date):\n",
    "    # get the weather forecast for a specific location and date\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&daily=weathercode,temperature_2m_max,temperature_2m_min,precipitation_sum,precipitation_hours,windspeed_10m_max,windgusts_10m_max&current_weather=true&timezone=GMT&start_date={date}&end_date={date}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also again define a helper function for counting the number of tokens in the message list. This will enable us to prevent exceeding the maximum number of tokens that the model can process in a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\") # load the encoding to use for counting the tokens. The \"cl100k_base\" encoding is compatibles with the OpenAI models gpt-4, gpt-3.5-turbo and text-embedding-ada-002\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a continuous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the example from the previous notebook, we will provide the model with a description of the `get_weather` function that we defined above using the [`functions` parameter](https://platform.openai.com/docs/guides/gpt/function-calling) of the OpenAI chat completions API. This way we can describe to the model what the function does and how it can be used.<br>\n",
    "We will also extend the system prompt to include the current date, so that the model is aware of it and can use it when retrieving weather information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define an object that describes our get_weather function to the GPT model in accordance with the OpenAI API specification\n",
    "get_weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the weather forecast for a specific location and date.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"latitude\": {\"type\": \"number\", \"description\": \"The latitude of the location (e.g. 12.345).\"},\n",
    "            \"longitude\": {\"type\": \"number\", \"description\": \"The longitude of the location (e.g. 12.345).\"},\n",
    "            \"date\": {\"type\": \"string\", \"description\": \"The date for which to get the weather forecast (YYYY-MM-DD).\"}\n",
    "        },\n",
    "        \"required\": [\"latitude\", \"longitude\", \"date\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the maximum number of tokens allowed in the conversation\n",
    "max_tokens = 3000\n",
    "\n",
    "# Define the system message, including the current date\n",
    "system_message = \"You are a helpful assistant. Current date is: \" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Initialize the message history with the system message\n",
    "messages = [{\"role\": \"system\", \"content\": system_message}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then include the description of the function when generating a response by using the `functions` parameter. Additionally, we will have to check if the model has called the function and if so, retrieve the weather data from the API and return it to the model. If the model has called a function, the `choices` object in the response will contain a `finish_reason` of `function_call` and the `message` will contain the name of the function and the parameters that were passed to it. The model is trained to return the parameters in a stringified JSON format - so we can parse the parameters from the model's response into a JSON object and call our function with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: What should I wear to work tomorrow?\n",
      "[ASSISTANT]: To provide you with accurate advice on what to wear to work tomorrow, I would need to know your location. Could you please provide me with the city or region where you will be working tomorrow?\n",
      "[USER]: I'm currently in paris.\n",
      "[FUNCTION]: get_weather(48.8566, 2.3522, 2023-09-08)\n",
      "[ASSISTANT]: The weather forecast for Paris tomorrow shows a weather code of 2, which indicates that it will be partly cloudy. The maximum temperature is expected to reach 34.6°C, while the minimum temperature will be around 19.5°C. The windspeed will be around 8.9 km/h. \n",
      "\n",
      "Based on this forecast, it is recommended to wear light and breathable clothing due to the warm temperatures. You may also want to bring a light jacket or sweater in case the temperature drops in the evening. Don't forget to stay hydrated and apply sunscreen if you'll be spending time outdoors.\n",
      "[USER]: exit\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import json\n",
    "\n",
    "# Read the first message from the user\n",
    "user_message = input(\"You: \")\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "print(\"[USER]: \" + user_message)\n",
    "\n",
    "# Create a conversation loop\n",
    "while user_message != \"exit\":   # The loop will run until the user types \"exit\"\n",
    "\n",
    "    # Manage the length of the message history\n",
    "    while num_tokens_from_messages(messages) > max_tokens:\n",
    "        messages.pop(1) # remove the oldest message (but keep the system message at index 0)\n",
    "    \n",
    "    # Generate a response from the GPT model\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id = \"gpt-35-turbo\",     # The deployment ID for your Azure OpenAI resource\n",
    "        messages = messages,\n",
    "        functions = [get_weather_function], # The function(s) that the model can call\n",
    "        temperature = 0.1\n",
    "    )\n",
    "\n",
    "    # Check if the response contains a function call\n",
    "    if response['choices'][0]['finish_reason'] == \"function_call\":\n",
    "        # Get the function call from the response\n",
    "        function_call = response['choices'][0]['message']['function_call']\n",
    "        if function_call['name'] == \"get_weather\":\n",
    "            # Parse the arguments for the function call from string to json\n",
    "            arguments = json.loads(function_call['arguments'])\n",
    "            # Call the function and get the response (in this case, a json object with the weather forecast)\n",
    "            weather = get_weather(arguments.get(\"latitude\"), arguments.get(\"longitude\"), arguments.get(\"date\"))\n",
    "            # Add the response to the message history\n",
    "            messages.append({\"role\": \"function\", \"name\": \"get_weather\", \"content\": json.dumps(weather)})\n",
    "            print(f\"[FUNCTION]: get_weather({arguments.get('latitude')}, {arguments.get('longitude')}, {arguments.get('date')})\")\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # Add the response to the message history\n",
    "        assistant_message = response['choices'][0]['message']['content']\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        print(\"[ASSISTANT]: \" + assistant_message)\n",
    "\n",
    "        # Read the next message from the user\n",
    "        sleep(1)\n",
    "        user_message = input(\"You: \")\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(\"[USER]: \" + user_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your time again to play around with the model and see what it can do! Can you think of other functions that you could provide to the model? What other tasks could you enable the model to perform?\n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
