{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e7451",
   "metadata": {},
   "source": [
    "# Creating basic chat completions\n",
    "\n",
    "Chat models are a class of GPT models that take a series of messages as input, and return a model-generated message as output. All newer GPT models released by OpenAI are chat models, including GPT-3.5-Turbo and GPT-4 (which are used in ChatGPT). In this notebook, we will show how to use these models to generate chat completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914026b",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa2258",
   "metadata": {},
   "source": [
    "First we need to install the `openai` library in your environment. You can do this by running the `pip` command below. Typically, you would do this in a terminal or command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade \"openai<1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea128b",
   "metadata": {},
   "source": [
    "Next we need to import the `openai` library so we can use it in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbb9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947500e",
   "metadata": {},
   "source": [
    "Now we can use the `openai` library to configure the API endpoint and API key that we will use to access the Azure OpenAI API.<br>\n",
    "Head over to [ai.cosmoconsult.com/me](https://ai.cosmoconsult.com/me) to get your personal ApiKey for this training series and paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1a2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for using an Azure OpenAI endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = \"<your_api_key>\"                           # The API key for your Azure OpenAI resource -> Get yours from https://ai.cosmoconsult.com/me\n",
    "openai.api_base = \"https://apis.ai.cosmoconsult.com\"        # The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_version = \"2023-07-01-preview\"                   # The API version for your Azure OpenAI resource. e.g. \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33f92a",
   "metadata": {},
   "source": [
    "## 2. Using an Azure OpenAI resource to generate chat completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9685631",
   "metadata": {},
   "source": [
    "After we have configured the API endpoint and API key that we want to use, we can now use the `openai.ChatCompletion` class to generate content with GPT-3.5-Turbo or GPT-4. This class simply acts as a wrapper around the OpenAI API, and allows us to more easily post requests to the API and parse the responses. But you can of course also [post requests to the API directly](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions), without using the `openai` library.\n",
    "\n",
    "The main input is the `messages` parameter. `messages` must be a list of message objects, where each object has a role (either `system`, `user`, or `assistant`) and content.\n",
    "- The first message is typically a `system` message, which contains global instructions and context for the model. You can use this to tell the model how to behave, how to interpret the user's messages or how to structure the response.\n",
    "- `user` messages typically contain user input or other information that the model should use to generate a response.\n",
    "- The models previous responses are stored as messages with the role `assistant`.\n",
    "\n",
    "You also have to specify the `deployment_id` parameter (or, when using the native OpenAI API, the `model` parameter). This is the ID of the model you want to use (e.g. `gpt-4`).\n",
    "\n",
    "Other than that, you can specify multiple optional parameters to control the behavior of the model. For example, you can specify the `max_tokens` parameter to control the length of the response, or the `temperature` parameter to control the creativity of the response. You can find a full list of parameters in the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions) (or in the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d67d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, and the first person to win two Nobel Prizes in different fields (physics and chemistry). Her discoveries led to the development of important medical technologies, including X-rays and radiation therapy for cancer. Curie was also a trailblazer for women in science, and her achievements continue to inspire generations of scientists around the world.\n"
     ]
    }
   ],
   "source": [
    "# a sample API call for chat completions looks as follows:\n",
    "try:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id = \"gpt-35-turbo\", # The deployment ID for your Azure OpenAI resource\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who was Marie Curie?\"},\n",
    "        ],\n",
    "        temperature = 0.2,              # Number between 0.0 and 2.0. Higher values will make the output more random, while lower values will make it more focused and deterministic. It is recommended altering this or top_p but not both.\n",
    "        n = 1,                          # How many completion choices to generate on each call.\n",
    "        stop = None,                    # One or more sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "        max_tokens = 150,               # The maximum number of tokens to generate.\n",
    "        stream = False,                 # Whether to stream back partial progress. If set, tokens will be returned one-by-one as they are generated, using multiple API calls if necessary.\n",
    "        # top_p = 1,                    # Number between 0.0 and 1.0. Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. It is recommended altering this or temperature but not both.\n",
    "        # presence_penalty = 0.0,       # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "        # frequency_penalty = 0.0,      # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "    )\n",
    "    # print the response\n",
    "    for choice in response['choices']:\n",
    "        print(choice['message']['content'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5e3b2",
   "metadata": {},
   "source": [
    "As you can see, the `response` we get from the API contains an array called `choices`, which in turn contains a `message` object with the content generated by the GPT model. Depending on the `n` parameter you set in your API call, you can get multiple `message` objects in the `choices` array. If you set `n=1`, the generated message will be the first (and only) element in the `choices` array - you can simply access it with `response.['choices'][0]['message']['content']`.\n",
    "\n",
    "The response also contains other information, such as the number of tokens processed or, if set up in the Azure OpenAI resource, information about potentially harmful content in the user input and in the content generated by the model. Have a look at the complete response to see what other information you can get from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11dc74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7wTE1zaqco7ImqLc8eMN1FgwMmvnF\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694169721,\n",
      "  \"model\": \"gpt-35-turbo\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Marie Curie was a Polish-born physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, and the first person to win two Nobel Prizes in different fields (physics and chemistry). Her discoveries led to the development of important medical technologies, including X-rays and radiation therapy for cancer. Curie was also a trailblazer for women in science, and her achievements continue to inspire generations of scientists around the world.\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 95,\n",
      "    \"prompt_tokens\": 25,\n",
      "    \"total_tokens\": 120\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98452e",
   "metadata": {},
   "source": [
    "Now let's get creative and generate some chat completions! Experiment with the parameters and see how they affect the output of the model. Try out different system and user messages, and see how the model responds to them. E.g., can you craft a system message that always creates a translation of the user's message, without any other content and no matter what the user says or asks? Or can you create a system message that always generates a response that contains a specific word or phrase?\n",
    "\n",
    "Happy experimenting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfe48de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
